/*
    This file is responsible for managing jobs.
    - Storing them on disc upon shutdown.
    - Scheduling and running jobs.
    - API for inserting and retrieving information from the frontend.

    Details:
        Create jobs

    Things to think about:
      - You should be able to move the jobsystem files to another directory.
        All paths should be relative for this to work.
*/

#import "Logger"
#import "Array"
#import "BucketArray"
#import "String"
#import "File"
#import "Stream"
#import "Math"

enum Timestamp : u64 {
    TIME_ZERO,
}
enum OutputHandle : i32 {
    HANDLE_ZERO,
}
#macro JOB_STATIC_SIZE 40 // 3 * 8 + 2*4
struct Job {
    id: u32;
    output_handle: OutputHandle;
    started: Timestamp;
    finished: Timestamp;
    created: Timestamp;

    // fields with varied length last, easier to serialize
    name: StringBuilder;
    description: StringBuilder;
}

struct OutputData {
    stdout_path: StringBuilder;
    raw_data: StringBuilder;
}
enum FilterSortType {
    SORT_NONE               =  0x0,
    SORT_REVERSE            =  0x1,
    SORT_NAME               =  0x2,
    SORT_TIMESTAMP_CREATED  =  0x4,
    SORT_TIMESTAMP_STARTED  =  0x8,
    SORT_TIMESTAMP_FINISHED = 0x10,
}
// One instance per runtime
struct JobSystem {
    jobs: BucketArray<Job>;
    outputs: Array<OutputData>;

    rootdir: StringBuilder; // derived from the directory the job system file was read from, it is not stored in the file itself because you would run into problems if you moved it.
    created: Timestamp;
    next_job_id: i32; // also the total number of created jobs

    // per session, not serialized
    started: Timestamp;
    initialized: bool;

    fn initialize(path: char[]) {
        if initialized {
            log("JobSystem already initialized!")
            return;
        }
        initialized = true;
        rootdir.append(path)

        job_path: StringBuilder // path to information of the job system
        job_path.append(rootdir)
        job_path.join_path("endercompute.dat")

        jobs.init(64)

        if(FileExist(job_path)) {
            yes := load_from_file(job_path)
            if !yes {
                log("Could not load ", job_path)
                return;
            }
        } else {
            // create new system
            log("Create new")
            created = cast_unsafe<Timestamp>StartMeasure();
        }

        started = cast_unsafe<Timestamp>StartMeasure();
    }

    fn create_job(name: char[], desc: char[]) {
        idx, job := jobs.add(null)
        
        job.id = next_job_id++
        job.name.append(name)
        job.description.append(desc)

        job.created = cast_unsafe<Timestamp>StartMeasure()
        
        file: StringBuilder
        defer file.cleanup()
        file.append(rootdir)
        file.append("/endercompute.dat")
        save_to_file(file)
    }

    fn find_job(name: char[]) -> Job* {
        iter := jobs.create_iterator()
        while jobs.iterate(&iter) {
            job := iter.ptr
            if job.name == name
                return job
        }
        return null
    }
    fn find_job(id: i32) -> Job* {
        iter := jobs.create_iterator()
        while jobs.iterate(&iter) {
            job := iter.ptr
            if job.id == id
                return job
        }
        return null
    }
    // don't forget to cleanup array
    fn filter_jobs(sort_type: FilterSortType, wildcard_name: char[] = {}, limit: i32 = 0) -> Array<Job*> {
        arr: Array<Job*>
        
        iter := jobs.create_iterator()
        if sort_type == SORT_REVERSE {
            while jobs.iterate_reverse(&iter) {
                job := iter.ptr
                if wildcard_name.len == 0 {
                    arr.add(job)
                } else {
                    at := find(wildcard_name, job.name)
                    if at != -1
                        arr.add(job)
                }
            }
        } else {
            while jobs.iterate(&iter) {
                job := iter.ptr
                if wildcard_name.len == 0 {
                    arr.add(job)
                } else {
                    at := find(wildcard_name, job.name)
                    if at != -1
                        arr.add(job)
                }
            }
        }
        fn cmp(a: Job*, b: Job*, sort_type: FilterSortType*) -> i32 {
            t := *sort_type
            diff: i64 = 0
            if t & SORT_NAME {
                head := 0
                diff = a.name.len - b.name.len // if all characters are equal then the length determines order
                while head < a.name.len && head < b.name.len {
                    chr_a := a.name[head]
                    chr_b := b.name[head]
                    if chr_a != chr_b {
                        diff = chr_a - chr_b
                        break
                    }
                    head++
                }
            } else if t & SORT_TIMESTAMP_CREATED {
                diff = cast<i64>a.created - cast<i64>b.created
            } else if t & SORT_TIMESTAMP_STARTED {
                diff = cast<i64>a.started - cast<i64>b.started
            } else if t & SORT_TIMESTAMP_FINISHED {
                diff = cast<i64>a.finished - cast<i64>b.finished
            }
            out: i32
            if diff > 0
                out = 1
            else if diff < 0
                out = -1
            return out
        }
        if sort_type > SORT_REVERSE
            sort(&arr, cast_unsafe<FnCmp>cmp, 0 != cast<i32>(sort_type & SORT_REVERSE), user_data = &sort_type)
        if limit > 0 && arr.size() > limit
            arr.resize(limit)
        return arr
    }

    fn save_to_file(path: char[]) -> bool {
        file := FileOpen(path, FILE_CLEAR_AND_WRITE)
        if !file
            return false;
        defer FileClose(file)

        stream: ByteStream

        header := cast<FileJobSystem_Header*>stream.write_late(sizeof FileJobSystem_Header)

        header.magic = HEADER_MAGIC
        header.version = 1

        header.job_count = jobs.getCount()
        header.output_count = outputs.size()
        header.next_job_id = next_job_id
        header.created = created

        header.job_offset = stream.get_write_head()

        iter := jobs.create_iterator()
        while jobs.iterate(&iter) {
            job := iter.ptr

            stream.write(job, JOB_STATIC_SIZE)

            stream.write<u8>(job.name.size())
            stream.write(job.name.data(), job.name.size())

            stream.write<u16>(job.description.size())
            stream.write(job.description.data(), job.description.size())
        }

        header.output_offset = stream.get_write_head()
        
        for 0..outputs.size() {
            output := outputs.get_unsafe(nr)

            stream.write<u16>(output.stdout_path.size())
            stream.write(output.stdout_path.data(), output.stdout_path.size())

            stream.write<i32>(output.raw_data.size())
            stream.write(output.raw_data.data(), output.raw_data.size())
        }

        iter2 := stream.create_iterator()
        while stream.iterate(&iter2)
            FileWrite(file, iter2.ptr, iter2.size)

        return true
    }
    fn load_from_file(path: char[]) -> bool {
        filesize: i64
        file := FileOpen(path, FILE_READ_ONLY, &filesize)
        if !file
            return false;
        defer FileClose(file)

        stream: ByteStream

        data := stream.write_late(filesize)
        if !data
            return false;
        defer Free(data)
        
        yes := FileRead(file, data, filesize)
        if !yes
            return false

        header := cast<FileJobSystem_Header*>data

        if header.magic != HEADER_MAGIC
            return false;
        
        if header.version != 1 {
            log("When reading job system from file: The version was ",header.version," but only version 1 is supported")
            return false;   
        }

        // if initialized {
        //     log("Cannot load file, job system already initialized")
        //     return false
        // }
        // initialized = true

        created = header.created
        next_job_id = header.next_job_id

        // jobs.resize(header.job_count)
        outputs.resize(header.output_count)

        read_head: i32 = header.job_offset

        for 0..header.job_count {
            _, job := jobs.add(null)

            stream.read(read_head, job, JOB_STATIC_SIZE)
            read_head += JOB_STATIC_SIZE

            len: u8;
            stream.read(read_head, &len, sizeof(len))
            read_head += sizeof(len)
            job.name.resize(len)
            stream.read(read_head, job.name.data(), len)
            read_head += len

            {
                len: u16;
                stream.read(read_head, &len, sizeof(len))
                read_head += sizeof(len)
                job.description.resize(len)
                stream.read(read_head, job.description.data(), len)
                read_head += len
            }
        }

        read_head = header.output_offset
        
        for 0..outputs.size() {
            output := outputs.get_unsafe(nr)

            len: u16;
            stream.read(read_head, &len, sizeof(len))
            read_head += sizeof(len)
            output.stdout_path.resize(len)
            stream.read(read_head, output.stdout_path.data(), len)
            read_head += len

            {
                len: u32;
                stream.read(read_head, &len, sizeof(len))
                read_head += sizeof(len)
                output.raw_data.resize(len)
                stream.read(read_head, output.raw_data.data(), len)
                read_head += len
            }
        }

        return true;
    }
}

fn InitJobSystem(path: char[]) -> JobSystem* {
    system := cast<JobSystem*>Allocate(sizeof JobSystem)
    construct(system)

    system.initialize(path)

    return system
}

// ######################
//    File formats
// ######################

// Job system stores:
//   - A file for the job system itself
//   - Files for the output of the jobs  

#macro HEADER_MAGIC 0x534a4345 // ECJS (ender compute job system)
struct FileJobSystem_Header {
    // NOTE: Fields with fixed size comes first

    // The same for every version
    magic: i32; // A constant
    version: i32;

    // Mostly the same for every version
    job_count: i32;
    job_offset: i32; // relative to start of file
    output_count: i32;
    output_offset: i32; // relative to start of file
    next_job_id: i32;
    created: Timestamp;

    // NOTE: Fields with varied size comes next

    // jobs: Job[job_count]
    // outputs: OutputData[output_count]
}

// ################
//  TESTS
// ################


fn create_test_jobs(system: JobSystem*, count: i32) {
    // Generated words from https://deepai.org/chat
    // Prompt: Can you create a list of 200 words related to names for a job system in continous integration. Can you format it to fit in a c++ array. Can you also mix in 1/3 normal words like "for", "in", "until".
    words: Slice<char>[] {
        "pipeline", "build", "test", "deploy", "merge", "commit", "branch", "repository", "integration", "automation", 
        "configuration", "trigger", "job", "run", "failure", "success", "status", "queue", "workspace", "workflow", 
        "version", "artifact", "release", "environment", "monitor", "dependencies", "script", "code", "execute", "validate", 
        "rollback", "publish", "notify", "report", "scheduler", "task", "link", "clone", "checkout", "dashboard", 
        "compile", "log", "status", "queue", "prioritize", "pipeline", "team", "collaborate", "integrate", "review", 
        "approval", "history", "manage", "archive", "backup", "branching", "synchronization", "permission", "failover", "retry", 
        "service", "upgrade", "monitor", "insight", "feedback", "matrix", "testbed", "commitment", "trigger", "agent", 
        "repository", "secure", "scan", "risk", "define", "evaluate", "optimize", "assess", "document", "template", 
        "export", "import", "customize", "package", "dependency", "snapshot", "timeout", "initialize", "complete", "for", 
        "in", "until", "from", "while", "and", "or", "with", "as", "to", "on", "at", "by", "that", 
        "if", "since", "so", "but", "about", "through", "during", "using", "against", "along", "after", 
        "before", "like", "within", "except", "without", "above", "below", "while", "each", "any", 
        "may", "could", "should", "once", "always", "never", "often", "again", "until", "greater", 
        "less", "equal", "from", "between", "among", "except", "alongside", "instead", "though", "however", 
        "although", "whether", "whereas", "neither", "either", "both", "some", "many", "few", "each", 
        "all", "common", "different", "full", "active", "passive", "automated", "manual", "novel", "existing", 
        "upcoming", "previous", "likely", "unlikely", "next"
    }
    for 0..count {
        name: StringBuilder
        desc: StringBuilder
        defer {name.cleanup() desc.cleanup() }
        name_n := random_range(3, 8)
        desc_n := random_range(5, 40)
        for 0..name_n {
            if nr != 0
                name.append(" ")
            ind := random_range(0, words.len)
            w := words[ind]
            name.append(w)
        }
        for 0..desc_n {
            if nr != 0
                desc.append(" ")
            ind := random_range(0, words.len)
            w := words[ind]
            desc.append(w)
        }
        system.create_job(name, desc)
    }
}

fn test_filter_jobs() {
    system := InitJobSystem("data")
    create_test_jobs(system, 30)
    
    arr := system.filter_jobs(SORT_NAME|SORT_REVERSE)
    for arr.sliced() {
        log(it.id, " ",it.name)
    }
}